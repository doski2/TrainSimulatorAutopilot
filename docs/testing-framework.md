# üß™ Framework de Testing - Train Simulator Autopilot

## Documentaci√≥n completa del sistema de testing para el proyecto Train

Simulator Autopilot

## üìã Resumen Ejecutivo

El framework de testing del Train Simulator Autopilot est√° dise√±ado para
garantizar la calidad, fiabilidad y robustez del sistema de piloto autom√°tico
inteligente. Utiliza **pytest** como framework principal con cobertura completa
de testing unitario, integraci√≥n y end-to-end.

## üèóÔ∏è Arquitectura del Framework

### Estructura de Tests

```text
tests/
‚îú‚îÄ‚îÄ unit/                    # Tests unitarios
‚îÇ   ‚îú‚îÄ‚îÄ test_tsc_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_predictive_telemetry.py
‚îú‚îÄ‚îÄ integration/            # Tests de integraci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ test_integration.py
‚îú‚îÄ‚îÄ e2e/                    # Tests end-to-end
‚îÇ   ‚îî‚îÄ‚îÄ test_e2e_scenarios.py
‚îî‚îÄ‚îÄ __init__.py
```

### Configuraci√≥n pytest

**Archivo: `pytest.ini`**

```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    --strict-markers
    --strict-config
    --cov=.
    --cov-report=term-missing
    --cov-report=html:htmlcov
markers =
    unit: Tests unitarios (marcados autom√°ticamente)
    integration: Tests de integraci√≥n entre componentes
    e2e: Tests end-to-end completos
    slow: Tests que requieren tiempo prolongado
    skip_ci: Tests a omitir en CI/CD
norecursedirs = scripts backup_* .*
```

## üìä Cobertura de Tests

### Resumen Ejecutivo

| Categor√≠a       | Cantidad     | Estado             | Cobertura | |
--------------- | ------------ | ------------------ | --------- | |
**Unitarios**   | 14 tests     | ‚úÖ Completo        | 85%+      | |
**Integraci√≥n** | 5 tests      | ‚úÖ Completo        | 90%+      | | **End-to-
End**  | 4 tests      | ‚úÖ Completo        | 95%+      | | **Total**       | **23
tests** | **100% Funcional** | **87%**   |

### Tests Unitarios (`tests/unit/`)

#### `test_tsc_integration.py`

**Prop√≥sito**: Validar la integraci√≥n individual con Train Simulator Classic

**Tests incluidos:**

- `test_initialization` - Inicializaci√≥n correcta del sistema
- `test_data_reading` - Lectura de datos de telemetr√≠a
- `test_command_sending` - Env√≠o de comandos al simulador
- `test_error_handling` - Manejo de errores de conexi√≥n
- `test_multi_locomotive_support` - Soporte multi-locomotora
- `test_connection_state_management` - Gesti√≥n de estados de conexi√≥n
- `test_data_validation` - Validaci√≥n de datos entrantes

**Marcadores**: `unit`

#### `test_predictive_telemetry.py`

**Prop√≥sito**: Validar el an√°lisis predictivo de telemetr√≠a

**Tests incluidos:**

- `test_model_initialization` - Inicializaci√≥n del modelo predictivo
- `test_data_processing` - Procesamiento de datos de telemetr√≠a
- `test_prediction_accuracy` - Precisi√≥n de predicciones
- `test_anomaly_detection` - Detecci√≥n de anomal√≠as
- `test_performance_metrics` - M√©tricas de rendimiento
- `test_real_time_processing` - Procesamiento en tiempo real
- `test_model_persistence` - Persistencia del modelo

**Marcadores**: `unit`, `slow`

### Tests de Integraci√≥n (`tests/integration/`)

#### `test_integration.py`

**Prop√≥sito**: Validar la interacci√≥n entre componentes del sistema

**Tests incluidos:**

1. **`test_tsc_data_flow`**
   - **Objetivo**: Verificar flujo completo de datos TSC
   - **Escenario**: Lectura ‚Üí Procesamiento ‚Üí Almacenamiento
   - **Validaciones**: Integridad de datos, timestamps, formato

2. **`test_command_execution_integration`**
   - **Objetivo**: Validar ejecuci√≥n de comandos entre componentes
   - **Escenario**: Comando IA ‚Üí TSC Integration ‚Üí Simulador
   - **Validaciones**: Ejecuci√≥n correcta, feedback, error handling

3. **`test_predictive_feedback_loop`**
   - **Objetivo**: Validar bucle de retroalimentaci√≥n predictiva
   - **Escenario**: Datos ‚Üí Predicci√≥n ‚Üí Acci√≥n ‚Üí Resultado
   - **Validaciones**: Consistencia, timing, accuracy

4. **`test_error_handling_integration`**
   - **Objetivo**: Probar manejo de errores entre componentes
   - **Escenario**: Error en un componente ‚Üí Propagaci√≥n ‚Üí Recuperaci√≥n
   - **Validaciones**: Logging, recovery, system stability

5. **`test_performance_integration`**
   - **Objetivo**: Validar rendimiento del sistema integrado
   - **Escenario**: Carga alta, m√∫ltiples locomotoras, predicciones continuas
   - **Validaciones**: Latencia, throughput, resource usage

**Marcadores**: `integration`, `slow`

### Tests End-to-End (`tests/e2e/`)

#### `test_e2e_scenarios.py`

**Prop√≥sito**: Validar escenarios completos de uso del sistema

**Tests incluidos:**

1. **`test_complete_driving_scenario`**
   - **Objetivo**: Simular conducci√≥n completa con piloto autom√°tico
   - **Escenario**: Inicio ‚Üí Aceleraci√≥n ‚Üí Mantenimiento velocidad ‚Üí Parada
   - **Validaciones**: Comandos correctos, timing, safety

2. **`test_emergency_stop_scenario`**
   - **Objetivo**: Validar respuesta a situaciones de emergencia
   - **Escenario**: Condici√≥n cr√≠tica ‚Üí Stop inmediato ‚Üí Recovery
   - **Validaciones**: Response time, safety protocols, logging

3. **`test_energy_efficiency_optimization`**
   - **Objetivo**: Optimizar eficiencia energ√©tica (sin gesti√≥n de combustible)
   - **Escenario**: An√°lisis predictivo ‚Üí Ajustes ‚Üí Mejora efficiency
   - **Validaciones**: Reducci√≥n consumo energ√≠a, mantenimiento performance

4. **`test_system_recovery_after_failure`**
   - **Objetivo**: Validar recuperaci√≥n tras fallos del sistema
   - **Escenario**: Failure ‚Üí Detection ‚Üí Recovery ‚Üí Normal operation
   - **Validaciones**: Downtime m√≠nimo, data integrity, system state

**Marcadores**: `e2e`, `slow`, `skip_ci`

## üöÄ Ejecuci√≥n de Tests

### Comandos B√°sicos

```bash
# Ejecutar todos los tests
python -m pytest

# Tests unitarios √∫nicamente
python -m pytest tests/unit/

# Tests de integraci√≥n
python -m pytest tests/integration/

# Tests end-to-end
python -m pytest tests/e2e/

# Con reporte de cobertura detallado
python -m pytest --cov=. --cov-report=html
```

### Ejecuci√≥n Selectiva

```bash
# Tests por marcador
python -m pytest -m "unit and not slow"    # Unitarios r√°pidos
python -m pytest -m integration           # Solo integraci√≥n
python -m pytest -m e2e                   # Solo end-to-end

# Tests espec√≠ficos
python -m pytest tests/unit/test_tsc_integration.py::TestTSCIntegration::test_initialization
python -m pytest tests/integration/test_integration.py -v

# Tests con salida verbosa
python -m pytest -v --tb=short
```

### Configuraci√≥n de Entorno

```bash
# Instalar dependencias de testing
pip install pytest pytest-cov pytest-mock

# Limpiar cache de tests
python -m pytest --cache-clear

# Ver configuraci√≥n
python -m pytest --collect-only --quiet
```

## üìà Reportes de Cobertura

### Generaci√≥n de Reportes

```bash
# Reporte en terminal
python -m pytest --cov=. --cov-report=term

# Reporte HTML
python -m pytest --cov=. --cov-report=html

# Reporte XML (para CI/CD)
python -m pytest --cov=. --cov-report=xml
```

### Interpretaci√≥n de Cobertura

- **L√≠neas ejecutadas**: C√≥digo efectivamente probado
- **Ramas (branches)**: Caminos condicionales probados
- **Funciones**: Funciones con al menos una ejecuci√≥n
- **Clases**: Clases instanciadas durante tests

### Umbrales de Calidad

- **Cobertura total**: ‚â• 80%
- **Cobertura de funciones cr√≠ticas**: ‚â• 90%
- **Cobertura de nuevos features**: ‚â• 85%
- **Ramas condicionales**: ‚â• 75%

## üõ†Ô∏è Herramientas y Utilidades

### Mocks y Fixtures

```python
import pytest
from unittest.mock import Mock, patch
from tests.fixtures import mock_tsc_data, mock_telemetry_stream

@pytest.fixture
def tsc_integration_mock():
    """Fixture para mock de TSC Integration"""
    with patch('tsc_integration.TSCIntegration') as mock:
        mock.return_value.obtener_datos_telemetria.return_value = mock_tsc_data()
        yield mock

@pytest.fixture
def telemetry_analyzer_mock():
    """Fixture para mock de Predictive Telemetry Analyzer"""
    with patch('predictive_telemetry_analysis.PredictiveTelemetryAnalyzer') as mock:
        yield mock
```

### Helpers de Testing

```python
def assert_telemetry_data_valid(data):
    """Valida estructura de datos de telemetr√≠a"""
    required_fields = ['speed', 'acceleration', 'brake_pressure', 'throttle']
    for field in required_fields:
        assert field in data, f"Campo requerido faltante: {field}"
        assert isinstance(data[field], (int, float)), f"Tipo incorrecto para {field}"

def simulate_tsc_response(delay=0.1):
    """Simula respuesta del TSC con delay configurable"""
    import time
    time.sleep(delay)
    return {"status": "success", "data": mock_tsc_data()}
```

## üîß Configuraci√≥n Avanzada

### pytest.ini - Configuraci√≥n Completa

```ini
[tool:pytest]
# Rutas de b√∫squeda
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Opciones por defecto
addopts =
    --strict-markers
    --strict-config
    --disable-warnings
    --tb=short
    -ra

# Marcadores personalizados
markers =
    unit: Tests unitarios b√°sicos
    integration: Tests de integraci√≥n entre componentes
    e2e: Tests end-to-end completos
    slow: Tests que requieren tiempo prolongado (>30s)
    skip_ci: Tests a omitir en entorno CI/CD
    smoke: Tests de humo para validaci√≥n r√°pida

# Exclusiones
norecursedirs =
    .git
    __pycache__
    .pytest_cache
    scripts
    backup_*
    .*

# Cobertura
[coverage:run]
source = .
omit =
    */tests/*
    */venv/*
    */__pycache__/*
    setup.py

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod
```

### Configuraci√≥n CI/CD

```yaml
# .github/workflows/test.yml
name: Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      - name: Run tests
        run: python -m pytest --cov=. --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
```

## üìä M√©tricas y KPIs

### M√©tricas de Calidad

- **Tasa de √©xito de tests**: ‚â• 99%
- **Tiempo de ejecuci√≥n**: < 5 minutos para suite completa
- **Cobertura de c√≥digo**: ‚â• 85%
- **Densidad de defects**: < 0.1 por 1000 l√≠neas

### M√©tricas de Rendimiento

- **Tests unitarios**: < 30 segundos
- **Tests integraci√≥n**: < 2 minutos
- **Tests E2E**: < 3 minutos
- **Memoria m√°xima**: < 500MB durante ejecuci√≥n

### Alertas y Monitoreo

- **Fallas consecutivas**: > 3 ‚Üí Alerta inmediata
- **Cobertura decreciente**: > 5% ‚Üí Revisi√≥n requerida
- **Tiempo ejecuci√≥n**: > 150% baseline ‚Üí Investigaci√≥n

## üêõ Debugging y Troubleshooting

### Problemas Comunes

#### Tests que fallan intermitentemente

```bash
# Ejecutar con m√°s verbosidad
python -m pytest -v -s --tb=long

# Ejecutar m√∫ltiples veces para verificar flake
python -m pytest --count=5 --maxfail=1
```

#### Problemas de cobertura

```bash
# Ver l√≠neas no cubiertas
python -m pytest --cov=. --cov-report=term-missing

# Excluir archivos espec√≠ficos
python -m pytest --cov=. --cov-report=html --cov-config=.coveragerc
```

#### Tests lentos

```bash
# Identificar tests lentos
python -m pytest --durations=10

# Ejecutar solo tests r√°pidos
python -m pytest -m "not slow"
```

### Logs y Debugging

```python
import logging

# Configurar logging para tests
@pytest.fixture(autouse=True)
def setup_logging():
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        filename='test_debug.log'
    )
```

## üìö Mejores Pr√°cticas

### Estructura Recomendada de Tests

```python
class TestFeature:
    """Tests para feature espec√≠fica"""

    def setup_method(self):
        """Setup antes de cada test"""
        self.system = SystemUnderTest()

    def teardown_method(self):
        """Cleanup despu√©s de cada test"""
        self.system.cleanup()

    def test_feature_normal_case(self):
        """Test caso normal"""
        # Given
        input_data = valid_input()

        # When
        result = self.system.process(input_data)

        # Then
        assert result.is_success()
        assert_telemetry_data_valid(result.data)

    def test_feature_edge_case(self):
        """Test caso borde"""
        # Given
        edge_input = edge_case_input()

        # When/Then
        with pytest.raises(ExpectedException):
            self.system.process(edge_input)
```

### Mocks y Stubs

```python
from unittest.mock import Mock, MagicMock

def test_with_mocks():
    """Ejemplo de test con mocks"""
    # Crear mocks
    tsc_mock = Mock()
    tsc_mock.obtener_datos_telemetria.return_value = mock_data()

    analyzer_mock = MagicMock()
    analyzer_mock.get_predictions.return_value = mock_predictions()

    # Inyectar mocks
    system = AutopilotSystem(tsc_mock, analyzer_mock)

    # Ejecutar test
    result = system.make_decision()

    # Verificar interacciones
    tsc_mock.obtener_datos_telemetria.assert_called_once()
    analyzer_mock.get_predictions.assert_called_once()

    # Verificar resultado
    assert result.action == "accelerate"
```

## üîÑ Mantenimiento del Framework

### Actualizaci√≥n de Tests

1. **Revisar cobertura** despu√©s de cambios en c√≥digo
2. **Actualizar mocks** cuando cambie la API
3. **Agregar tests** para nueva funcionalidad
4. **Refactorizar tests** cuando el c√≥digo cambie significativamente

### Limpieza Peri√≥dica

```bash
# Limpiar cache
find . -name "__pycache__" -type d -exec rm -rf {} +
find . -name "*.pyc" -delete
rm -rf .pytest_cache/
rm -rf htmlcov/

# Verificar estructura
python -m pytest --collect-only
```

### Actualizaci√≥n de Dependencias

```bash
# Actualizar pytest y plugins
pip install --upgrade pytest pytest-cov pytest-mock

# Verificar compatibilidad
python -m pytest --version
```

## üìû Soporte y Contacto

### Reportar Issues

- **Tests que fallan**: Crear issue con logs completos
- **Cobertura insuficiente**: Revisar y agregar tests faltantes
- **Performance issues**: Profile y optimizar tests lentos

### Documentaci√≥n Relacionada

- `docs/integration.md` - Gu√≠a de integraci√≥n del sistema
- `docs/ia-spec.md` - Especificaciones de IA
- `README.md` - Documentaci√≥n general del proyecto

---

üß™ Framework de testing completo y validado para garantizar la calidad del Train
Simulator Autopilot
